import os
import pathlib
import numpy as np
import pandas as pd
import streamlit as st
import textattack.models.wrappers
import torch
import subprocess
import transformers
import zipfile

from fastapi import HTTPException, APIRouter
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.encoders import jsonable_encoder
from datetime import datetime
from config import trainer, STREAMLIT_STATIC_PATH, DOWNLOAD_PATH
from transformers import AutoTokenizer, AutoModelForSequenceClassification

router = APIRouter(prefix='/endpoints',
                   tags=['trainer'],
                   responses={200: {'description': 'OK'},
                              404: {'description': 'Resource Not Found'},
                              415: {'description': 'Unsupported Media Type'}})

path = os.getcwd()


@router.post('/trainer')
def trainer(model_name_or_path: str, dataset: str, attack: str, task_type: str = 'classification',
            model_max_length: str = None, model_num_labels: int = None,
            dataset_train_split: float = None,
            dataset_eval_split: float = None,
            filter_train_by_labels: str = None,
            filter_eval_by_labels: str = None, num_epochs: int = 3,
            num_clean_epochs: int = 1, attack_epoch_interval: int = 1,
            early_stopping_epochs: int = None, learning_rate: float = 5e-5,
            num_warmup_steps: int = 500, weight_decay: float = 0.01,
            per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 32,
            gradient_accumulation_steps: int = 1, random_seed: int = 786,
            parallel: bool = False, load_best_model_at_end: bool = False,
            alpha: float = 1.0, num_train_adv_examples: int = -1,
            query_budget_train: float = None,
            attack_num_workers_per_device: int = 1, output_dir: str = './output',
            checkpoint_interval_steps: float = None, checkpoint_interval_epochs: int = None,
            save_last: bool = True, log_to_tb: bool = False,
            tb_log_dir: str = None, log_to_wandb: bool = False,
            wandb_project: str = 'textattack', logging_interval_step: int = 1):
    """
    This function is used to call the textattack CLI to run model training using the target system
    
    
    **model_name_or_path**: Name of the model to use or path to the model on the system

    **dataset**: Name of the dataset to use or the Dataset object generated by the user

    **attack**: Attack string

    **task_type**: Action to take while training

    **model_max_length**: Model Max Length

    **model_num_labels**: Number of Labels

    **dataset_train_split**: Train split for dataset

    **dataset_eval_split**: Evaluation split for dataset

    **filter_train_by_labels**: Filter Train Data By Labels

    **filter_eval_by_labels**: Filter Evaluation Data By Labels

    **num_epochs**: Total number of epochs for training

    **num_clean_epochs**: Number of epochs to train on just the original training dataset before adversarial training

    **attack_epoch_interval**: Generate a new adversarial training set every N epochs

    **early_stopping_epochs**: Number of epochs validation must increase before stopping early

    **learning_rate**: Learning rate of the model

    **num_warmup_steps**: The number of steps for the warmup phase of linear scheduler

    **weight_decay**: Weight decay (L2 penalty)

    **per_device_train_batch_size**: The batch size per GPU/CPU for training

    **per_device_eval_batch_size**: The batch size per GPU/CPU for evaluation

    **gradient_accumulation_steps**: Number of updates steps to accumulate the gradients before performing a
                                     backward/update pass

    **random_seed**: Random seed for reproducibility

    **parallel**: Use Multiple GPUs using torch.DataParallel class

    **load_best_model_at_end**: keep track of the best model across training and load it at the end

    **alpha**: The weight for adversarial loss

    **num_train_adv_examples**: The number of samples to successfully attack when generating adversarial training
                                set before start of every epoch

    **query_budget_train**: The max query budget to use when generating adversarial training set

    **attack_num_workers_per_device**: Number of worker processes to run per device for attack

    **output_dir**: Directory to output training logs and checkpoints

    **checkpoint_interval_steps**: Save after N updates

    **checkpoint_interval_epochs**: Save after N epochs

    **save_last**: Save the model at end of training

    **log_to_tb**: Log to Tensorboard

    **tb_log_dir**: Directory to output training logs and checkpoints

    **log_to_wandb**: Log to Wandb

    **wandb_project**: Name of Wandb project for logging

    **logging_interval_step**: Log to Tensorboard/Wandb every N training steps
    """
    
    var_list = ['textattack', 'train']
    maps = {
        'model_name_or_path': ['--model-name-or-path', model_name_or_path],
        'dataset': ['--dataset', dataset],
        'attack': ['--attack', attack],
        'task_type': ['--task-type', task_type],
        'model_max_length': ['--model-max-length', model_max_length],
        'model_num_labels': ['--model-num-labels', model_num_labels],
        'dataset_train_split': ['--dataset-train-split', dataset_train_split],
        'dataset_eval_split': ['--dataset-eval-split', dataset_eval_split],
        'filter_train_by_labels': ['--filter-train-by-labels', filter_train_by_labels],
        'filter_eval_by_labels': ['--filter-eval-by-labels', filter_eval_by_labels],
        'num_epochs': ['--num-epochs', num_epochs],
        'num_clean_epochs': ['--num-clean-epochs', num_clean_epochs],
        'attack_epoch_interval': ['--attack-epoch-interval', attack_epoch_interval],
        'early_stopping_epochs': ['--early-stopping-epochs', early_stopping_epochs],
        'learning_rate': ['--learning-rate', learning_rate],
        'num_warmup_steps': ['--num-warmup-steps', num_warmup_steps],
        'weight_decay': ['--weight-decay', weight_decay],
        'per_device_train_batch_size': ['--per-device-train-batch-size', per_device_train_batch_size],
        'per_device_eval_batch_size': ['--per-device-eval-batch-size', per_device_eval_batch_size],
        'gradient_accumulation_steps': ['--gradient-accumulation-steps', gradient_accumulation_steps],
        'random_seed': ['--random-seed', random_seed],
        'parallel': ['--parallel', parallel],
        'load_best_model_at_end': ['--load-best-model-at-end', load_best_model_at_end],
        'alpha': ['--alpha', alpha],
        'num_train_adv_examples': ['--num-train-adv-examples', num_train_adv_examples],
        'query_budget_train': ['--query-budget-train', query_budget_train],
        'attack_num_workers_per_device': ['--attack-num-workers-per-device', attack_num_workers_per_device],
        'output_dir': ['--output-dir', output_dir],
        'checkpoint_interval_steps': ['--checkpoint-interval-steps', checkpoint_interval_steps],
        'checkpoint_interval_epochs': ['--checkpoint-interval-epochs', checkpoint_interval_epochs],
        'save_last': ['--save-last', save_last],
        'log_to_tb': ['--log-to-tb', log_to_tb],
        'tb_log_dir': ['--tb-log-dir', tb_log_dir],
        'log_to_wandb': ['--log-to-wandb', log_to_wandb],
        'wandb_project': ['--wandb-project', wandb_project],
        'logging_interval_step': ['--logging-interval-step', logging_interval_step]
    }
    maps = {key: value for key, value in maps.items() if value[1] is not None}
    for k, v in maps.items():
        var_list.extend(v)

    var_list = [str(iter_) for iter_ in var_list if type(iter_) is not bool]

    try:
        subprocess.run(var_list)
    except Exception as ex:
        raise HTTPException(status_code=415, detail=ex)
    else:
        if output_dir is not None:
            try:
                with zipfile.ZipFile('file.zip', 'w') as zipped:
                    for folder, subfolder, fnames in os.walk(output_dir):
                        for fname in fnames:
                            fpath = os.path.join(folder, fname)
                            zipped.write(fpath, os.path.basename(fpath))
            except Exception as ex:
                raise HTTPException(status_code=404, detail=ex)
            else:
                return FileResponse('file.zip', media_type='application/zip', filename='file.zip')
